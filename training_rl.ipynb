{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CybORG import CybORG\n",
    "from CybORG.Simulator.Scenarios import EnterpriseScenarioGenerator\n",
    "from CybORG.Agents.Wrappers import EnterpriseMAE\n",
    "from CybORG.Agents import SleepAgent, EnterpriseGreenAgent, FiniteStateRedAgent\n",
    "from Wrappers import EnterpriseMAEMaskWrapper, MaskWrapper\n",
    "from CustomRLLib import TorchActionMaskModel, CustomModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.ppo import PPOConfig, PPO\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator_CC4(env_config: dict):\n",
    "    sg = EnterpriseScenarioGenerator(\n",
    "        blue_agent_class=SleepAgent,\n",
    "        green_agent_class=EnterpriseGreenAgent,\n",
    "        red_agent_class=FiniteStateRedAgent,\n",
    "        steps=500\n",
    "    )\n",
    "    cyborg = CybORG(scenario_generator=sg)\n",
    "    # env = EnterpriseMAE(env=cyborg)\n",
    "    # env = EnterpriseMAEMaskWrapper(env=cyborg)\n",
    "    env = MaskWrapper(env=cyborg)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env(name='CC4', env_creator=lambda config: env_creator_CC4(config))\n",
    "env = env_creator_CC4({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGENTS = 5\n",
    "POLICY_MAP = {f\"blue_agent_{i}\": f\"Agent{i}\" for i in range(NUM_AGENTS)}\n",
    "\n",
    "def policy_mapper(agent_id, episode, worker, **kwargs):\n",
    "# def policy_mapper(agent_id, episode, **kwargs):\n",
    "    return POLICY_MAP[agent_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"custom_pytorch_model\", CustomModel)\n",
    "ModelCatalog.register_custom_model(\"torch_action_mask_model\", TorchActionMaskModel)\n",
    "# ModelCatalog.register_custom_model(\"action_mask_model\", ActionMaskModel)\n",
    "\n",
    "\n",
    "# ray.init()\n",
    "# algo = ppo.PPO(env=\"CartPole-v1\", config={\n",
    "#     \"model\": {\n",
    "#         \"custom_model\": \"my_tf_model\",\n",
    "#         # Extra kwargs to be passed to your model's c'tor.\n",
    "#         \"custom_model_config\": {},\n",
    "#     },\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"CC4\")\n",
    "    .framework(\"torch\")\n",
    "    .debugging(logger_config={'logdir':'./logs/Masked_PPO_Example', 'type': 'ray.tune.logger.TBXLogger'})\n",
    "    .multi_agent(policies={\n",
    "        ray_agent: PolicySpec(\n",
    "            policy_class=None,\n",
    "            observation_space=env.observation_space(cyborg_agent),\n",
    "            action_space=env.action_space(cyborg_agent),\n",
    "            config={'gamma': 0.85}\n",
    "        ) for cyborg_agent, ray_agent in POLICY_MAP.items()\n",
    "    },\n",
    "    policy_mapping_fn=policy_mapper\n",
    "    )\n",
    "    .training(\n",
    "        model={'custom_model': \"torch_action_mask_model\"}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(algo_config.policies['Agent0'].policy_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-16_23:54:54\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d_%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/cage-4/lib/python3.10/site-packages/ray/rllib/utils/from_config.py:197: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  object_ = constructor(*ctor_args, **ctor_kwargs)\n",
      "2024-04-17 01:29:19,648\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghob9o3gUj+JRTAgDAJnVaEo=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghgb7UTyKR/EoJoQBlgviuw==', 'shape': (267,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'dict', 'spaces': {'action_mask': {'space': 'multi-binary', 'n': 82}, 'observations': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghob9o3gUj+JRTAgDAJnVaEo=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghgb7UTyKR/EoJoQBlgviuw==', 'shape': (267,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'dict', 'spaces': {'action_mask': {'space': 'multi-binary', 'n': 82}, 'observations': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghob9o3gUj+JRTAgDAJnVaEo=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghgb7UTyKR/EoJoQBlgviuw==', 'shape': (267,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'dict', 'spaces': {'action_mask': {'space': 'multi-binary', 'n': 82}, 'observations': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghob9o3gUj+JRTAgDAJnVaEo=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghgb7UTyKR/EoJoQBlgviuw==', 'shape': (267,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'dict', 'spaces': {'action_mask': {'space': 'multi-binary', 'n': 82}, 'observations': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpmZsY6mjoKtQpkAy4Ghob9o3gUj+JRPIpH8SgexaN4FFMDAwDaXlXc', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpmZsY6mjoKtQpkAy4Ghgb7UTyKR/EoHsWjeBSP4lE8iqmBAbBSCk0=', 'shape': (663,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'dict', 'spaces': {'action_mask': {'space': 'multi-binary', 'n': 242}, 'observations': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGhgY6mjoKtQpkAy5mBghgGqVH6VF6lB6lR+kBpAFVox0m', 'dtype': '<i8'}}}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResult(checkpoint=Checkpoint(filesystem=local, path=./Submissions/mask_results_50_2024-04-17_02:24:21/staging/), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'Agent3': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 8.562258143598834, 'cur_kl_coeff': 0.6750000000000002, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.699180987974009, 'policy_loss': -0.061590733847212205, 'vf_loss': 8.7496846879522, 'vf_explained_var': 0.5017867753903071, 'kl': 0.016425228798326495, 'entropy': 3.0372680140038333, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 47520.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent0': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 7.050541699305176, 'cur_kl_coeff': 1.0124999999999997, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.759998994568983, 'policy_loss': -0.04837757342271895, 'vf_loss': 8.797991787890593, 'vf_explained_var': 0.5633485648160179, 'kl': 0.010256551129850848, 'entropy': 2.9815073157350223, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 47520.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent1': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 5.923391640310486, 'cur_kl_coeff': 1.0124999999999997, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.895461032787958, 'policy_loss': -0.04730633878934896, 'vf_loss': 8.930259844660759, 'vf_explained_var': 0.389697879490753, 'kl': 0.012353088909818217, 'entropy': 3.0438847531874975, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 47520.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent2': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 9.130747334783276, 'cur_kl_coeff': 1.0124999999999997, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.93010552028815, 'policy_loss': -0.05286036518276281, 'vf_loss': 8.971728058656057, 'vf_explained_var': 0.5304285041987896, 'kl': 0.011099071273614777, 'entropy': 3.2246227897703648, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 47520.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent4': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 6.792795273909966, 'cur_kl_coeff': 1.0124999999999997, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.571877094606558, 'policy_loss': -0.09226280700095231, 'vf_loss': 8.649083054065704, 'vf_explained_var': 0.4638243606934945, 'kl': 0.014870911715703036, 'entropy': 3.9960804397861165, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 47520.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 200000, 'num_env_steps_trained': 200000, 'num_agent_steps_sampled': 1000000, 'num_agent_steps_trained': 1000000}, 'sampler_results': {'episode_reward_max': -7000.0, 'episode_reward_min': -33725.0, 'episode_reward_mean': -19087.7, 'episode_len_mean': 499.0, 'episode_media': {}, 'episodes_this_iter': 8, 'policy_reward_min': {'Agent0': -6745.0, 'Agent3': -6745.0, 'Agent2': -6745.0, 'Agent4': -6745.0, 'Agent1': -6745.0}, 'policy_reward_max': {'Agent0': -1400.0, 'Agent3': -1400.0, 'Agent2': -1400.0, 'Agent4': -1400.0, 'Agent1': -1400.0}, 'policy_reward_mean': {'Agent0': -3817.54, 'Agent3': -3817.54, 'Agent2': -3817.54, 'Agent4': -3817.54, 'Agent1': -3817.54}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-19380.0, -18110.0, -22100.0, -15405.0, -24200.0, -20575.0, -16375.0, -14260.0, -14945.0, -19065.0, -13285.0, -21475.0, -10550.0, -21910.0, -13270.0, -17240.0, -26360.0, -21855.0, -17890.0, -20715.0, -16530.0, -21580.0, -19770.0, -14410.0, -7000.0, -18650.0, -24460.0, -19395.0, -26740.0, -18840.0, -17980.0, -23785.0, -22440.0, -18900.0, -17850.0, -17375.0, -33725.0, -16580.0, -20295.0, -24995.0, -13605.0, -13605.0, -13520.0, -21595.0, -19150.0, -9725.0, -18190.0, -14640.0, -28195.0, -21185.0, -26650.0, -21415.0, -18280.0, -20695.0, -17305.0, -18050.0, -20890.0, -21955.0, -16670.0, -25965.0, -20630.0, -13485.0, -13700.0, -15770.0, -18665.0, -14535.0, -25565.0, -21985.0, -19280.0, -21430.0, -13700.0, -18300.0, -16775.0, -16305.0, -19435.0, -24025.0, -26595.0, -14425.0, -15910.0, -19720.0, -12265.0, -10555.0, -21705.0, -23985.0, -17560.0, -20590.0, -19335.0, -18855.0, -11925.0, -22610.0, -18135.0, -27200.0, -16555.0, -23910.0, -17405.0, -19250.0, -20095.0, -22620.0, -27305.0, -15125.0], 'episode_lengths': [499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499], 'policy_Agent0_reward': [-3876.0, -3622.0, -4420.0, -3081.0, -4840.0, -4115.0, -3275.0, -2852.0, -2989.0, -3813.0, -2657.0, -4295.0, -2110.0, -4382.0, -2654.0, -3448.0, -5272.0, -4371.0, -3578.0, -4143.0, -3306.0, -4316.0, -3954.0, -2882.0, -1400.0, -3730.0, -4892.0, -3879.0, -5348.0, -3768.0, -3596.0, -4757.0, -4488.0, -3780.0, -3570.0, -3475.0, -6745.0, -3316.0, -4059.0, -4999.0, -2721.0, -2721.0, -2704.0, -4319.0, -3830.0, -1945.0, -3638.0, -2928.0, -5639.0, -4237.0, -5330.0, -4283.0, -3656.0, -4139.0, -3461.0, -3610.0, -4178.0, -4391.0, -3334.0, -5193.0, -4126.0, -2697.0, -2740.0, -3154.0, -3733.0, -2907.0, -5113.0, -4397.0, -3856.0, -4286.0, -2740.0, -3660.0, -3355.0, -3261.0, -3887.0, -4805.0, -5319.0, -2885.0, -3182.0, -3944.0, -2453.0, -2111.0, -4341.0, -4797.0, -3512.0, -4118.0, -3867.0, -3771.0, -2385.0, -4522.0, -3627.0, -5440.0, -3311.0, -4782.0, -3481.0, -3850.0, -4019.0, -4524.0, -5461.0, -3025.0], 'policy_Agent3_reward': [-3876.0, -3622.0, -4420.0, -3081.0, -4840.0, -4115.0, -3275.0, -2852.0, -2989.0, -3813.0, -2657.0, -4295.0, -2110.0, -4382.0, -2654.0, -3448.0, -5272.0, -4371.0, -3578.0, -4143.0, -3306.0, -4316.0, -3954.0, -2882.0, -1400.0, -3730.0, -4892.0, -3879.0, -5348.0, -3768.0, -3596.0, -4757.0, -4488.0, -3780.0, -3570.0, -3475.0, -6745.0, -3316.0, -4059.0, -4999.0, -2721.0, -2721.0, -2704.0, -4319.0, -3830.0, -1945.0, -3638.0, -2928.0, -5639.0, -4237.0, -5330.0, -4283.0, -3656.0, -4139.0, -3461.0, -3610.0, -4178.0, -4391.0, -3334.0, -5193.0, -4126.0, -2697.0, -2740.0, -3154.0, -3733.0, -2907.0, -5113.0, -4397.0, -3856.0, -4286.0, -2740.0, -3660.0, -3355.0, -3261.0, -3887.0, -4805.0, -5319.0, -2885.0, -3182.0, -3944.0, -2453.0, -2111.0, -4341.0, -4797.0, -3512.0, -4118.0, -3867.0, -3771.0, -2385.0, -4522.0, -3627.0, -5440.0, -3311.0, -4782.0, -3481.0, -3850.0, -4019.0, -4524.0, -5461.0, -3025.0], 'policy_Agent2_reward': [-3876.0, -3622.0, -4420.0, -3081.0, -4840.0, -4115.0, -3275.0, -2852.0, -2989.0, -3813.0, -2657.0, -4295.0, -2110.0, -4382.0, -2654.0, -3448.0, -5272.0, -4371.0, -3578.0, -4143.0, -3306.0, -4316.0, -3954.0, -2882.0, -1400.0, -3730.0, -4892.0, -3879.0, -5348.0, -3768.0, -3596.0, -4757.0, -4488.0, -3780.0, -3570.0, -3475.0, -6745.0, -3316.0, -4059.0, -4999.0, -2721.0, -2721.0, -2704.0, -4319.0, -3830.0, -1945.0, -3638.0, -2928.0, -5639.0, -4237.0, -5330.0, -4283.0, -3656.0, -4139.0, -3461.0, -3610.0, -4178.0, -4391.0, -3334.0, -5193.0, -4126.0, -2697.0, -2740.0, -3154.0, -3733.0, -2907.0, -5113.0, -4397.0, -3856.0, -4286.0, -2740.0, -3660.0, -3355.0, -3261.0, -3887.0, -4805.0, -5319.0, -2885.0, -3182.0, -3944.0, -2453.0, -2111.0, -4341.0, -4797.0, -3512.0, -4118.0, -3867.0, -3771.0, -2385.0, -4522.0, -3627.0, -5440.0, -3311.0, -4782.0, -3481.0, -3850.0, -4019.0, -4524.0, -5461.0, -3025.0], 'policy_Agent4_reward': [-3876.0, -3622.0, -4420.0, -3081.0, -4840.0, -4115.0, -3275.0, -2852.0, -2989.0, -3813.0, -2657.0, -4295.0, -2110.0, -4382.0, -2654.0, -3448.0, -5272.0, -4371.0, -3578.0, -4143.0, -3306.0, -4316.0, -3954.0, -2882.0, -1400.0, -3730.0, -4892.0, -3879.0, -5348.0, -3768.0, -3596.0, -4757.0, -4488.0, -3780.0, -3570.0, -3475.0, -6745.0, -3316.0, -4059.0, -4999.0, -2721.0, -2721.0, -2704.0, -4319.0, -3830.0, -1945.0, -3638.0, -2928.0, -5639.0, -4237.0, -5330.0, -4283.0, -3656.0, -4139.0, -3461.0, -3610.0, -4178.0, -4391.0, -3334.0, -5193.0, -4126.0, -2697.0, -2740.0, -3154.0, -3733.0, -2907.0, -5113.0, -4397.0, -3856.0, -4286.0, -2740.0, -3660.0, -3355.0, -3261.0, -3887.0, -4805.0, -5319.0, -2885.0, -3182.0, -3944.0, -2453.0, -2111.0, -4341.0, -4797.0, -3512.0, -4118.0, -3867.0, -3771.0, -2385.0, -4522.0, -3627.0, -5440.0, -3311.0, -4782.0, -3481.0, -3850.0, -4019.0, -4524.0, -5461.0, -3025.0], 'policy_Agent1_reward': [-3876.0, -3622.0, -4420.0, -3081.0, -4840.0, -4115.0, -3275.0, -2852.0, -2989.0, -3813.0, -2657.0, -4295.0, -2110.0, -4382.0, -2654.0, -3448.0, -5272.0, -4371.0, -3578.0, -4143.0, -3306.0, -4316.0, -3954.0, -2882.0, -1400.0, -3730.0, -4892.0, -3879.0, -5348.0, -3768.0, -3596.0, -4757.0, -4488.0, -3780.0, -3570.0, -3475.0, -6745.0, -3316.0, -4059.0, -4999.0, -2721.0, -2721.0, -2704.0, -4319.0, -3830.0, -1945.0, -3638.0, -2928.0, -5639.0, -4237.0, -5330.0, -4283.0, -3656.0, -4139.0, -3461.0, -3610.0, -4178.0, -4391.0, -3334.0, -5193.0, -4126.0, -2697.0, -2740.0, -3154.0, -3733.0, -2907.0, -5113.0, -4397.0, -3856.0, -4286.0, -2740.0, -3660.0, -3355.0, -3261.0, -3887.0, -4805.0, -5319.0, -2885.0, -3182.0, -3944.0, -2453.0, -2111.0, -4341.0, -4797.0, -3512.0, -4118.0, -3867.0, -3771.0, -2385.0, -4522.0, -3627.0, -5440.0, -3311.0, -4782.0, -3481.0, -3850.0, -4019.0, -4524.0, -5461.0, -3025.0]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.766215894479135, 'mean_inference_ms': 1.9107422531764942, 'mean_action_processing_ms': 0.1736246614642831, 'mean_env_wait_ms': 19.216977280063816, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.01610221862792969, 'StateBufferConnector_ms': 0.0017731189727783203, 'ViewRequirementAgentConnector_ms': 0.03594498634338379}}, 'episode_reward_max': -7000.0, 'episode_reward_min': -33725.0, 'episode_reward_mean': -19087.7, 'episode_len_mean': 499.0, 'episodes_this_iter': 8, 'policy_reward_min': {'Agent0': -6745.0, 'Agent3': -6745.0, 'Agent2': -6745.0, 'Agent4': -6745.0, 'Agent1': -6745.0}, 'policy_reward_max': {'Agent0': -1400.0, 'Agent3': -1400.0, 'Agent2': -1400.0, 'Agent4': -1400.0, 'Agent1': -1400.0}, 'policy_reward_mean': {'Agent0': -3817.54, 'Agent3': -3817.54, 'Agent2': -3817.54, 'Agent4': -3817.54, 'Agent1': -3817.54}, 'hist_stats': {'episode_reward': [-19380.0, -18110.0, -22100.0, -15405.0, -24200.0, -20575.0, -16375.0, -14260.0, -14945.0, -19065.0, -13285.0, -21475.0, -10550.0, -21910.0, -13270.0, -17240.0, -26360.0, -21855.0, -17890.0, -20715.0, -16530.0, -21580.0, -19770.0, -14410.0, -7000.0, -18650.0, -24460.0, -19395.0, -26740.0, -18840.0, -17980.0, -23785.0, -22440.0, -18900.0, -17850.0, -17375.0, -33725.0, -16580.0, -20295.0, -24995.0, -13605.0, -13605.0, -13520.0, -21595.0, -19150.0, -9725.0, -18190.0, -14640.0, -28195.0, -21185.0, -26650.0, -21415.0, -18280.0, -20695.0, -17305.0, -18050.0, -20890.0, -21955.0, -16670.0, -25965.0, -20630.0, -13485.0, -13700.0, -15770.0, -18665.0, -14535.0, -25565.0, -21985.0, -19280.0, -21430.0, -13700.0, -18300.0, -16775.0, -16305.0, -19435.0, -24025.0, -26595.0, -14425.0, -15910.0, -19720.0, -12265.0, -10555.0, -21705.0, -23985.0, -17560.0, -20590.0, -19335.0, -18855.0, -11925.0, -22610.0, -18135.0, -27200.0, -16555.0, -23910.0, -17405.0, -19250.0, -20095.0, -22620.0, -27305.0, -15125.0], 'episode_lengths': [499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499], 'policy_Agent0_reward': [-3876.0, -3622.0, -4420.0, -3081.0, -4840.0, -4115.0, -3275.0, -2852.0, -2989.0, -3813.0, -2657.0, -4295.0, -2110.0, -4382.0, -2654.0, -3448.0, -5272.0, -4371.0, -3578.0, -4143.0, -3306.0, -4316.0, -3954.0, -2882.0, -1400.0, -3730.0, -4892.0, -3879.0, -5348.0, -3768.0, -3596.0, -4757.0, -4488.0, -3780.0, -3570.0, -3475.0, -6745.0, -3316.0, -4059.0, -4999.0, -2721.0, -2721.0, -2704.0, -4319.0, -3830.0, -1945.0, -3638.0, -2928.0, -5639.0, -4237.0, -5330.0, -4283.0, -3656.0, -4139.0, -3461.0, -3610.0, -4178.0, -4391.0, -3334.0, -5193.0, -4126.0, -2697.0, -2740.0, -3154.0, -3733.0, -2907.0, -5113.0, -4397.0, -3856.0, -4286.0, -2740.0, -3660.0, -3355.0, -3261.0, -3887.0, -4805.0, -5319.0, -2885.0, -3182.0, -3944.0, -2453.0, -2111.0, -4341.0, -4797.0, -3512.0, -4118.0, -3867.0, -3771.0, -2385.0, -4522.0, -3627.0, -5440.0, -3311.0, -4782.0, -3481.0, -3850.0, -4019.0, -4524.0, -5461.0, -3025.0], 'policy_Agent3_reward': [-3876.0, -3622.0, -4420.0, -3081.0, -4840.0, -4115.0, -3275.0, -2852.0, -2989.0, -3813.0, -2657.0, -4295.0, -2110.0, -4382.0, -2654.0, -3448.0, -5272.0, -4371.0, -3578.0, -4143.0, -3306.0, -4316.0, -3954.0, -2882.0, -1400.0, -3730.0, -4892.0, -3879.0, -5348.0, -3768.0, -3596.0, -4757.0, -4488.0, -3780.0, -3570.0, -3475.0, -6745.0, -3316.0, -4059.0, -4999.0, -2721.0, -2721.0, -2704.0, -4319.0, -3830.0, -1945.0, -3638.0, -2928.0, -5639.0, -4237.0, -5330.0, -4283.0, -3656.0, -4139.0, -3461.0, -3610.0, -4178.0, -4391.0, -3334.0, -5193.0, -4126.0, -2697.0, -2740.0, -3154.0, -3733.0, -2907.0, -5113.0, -4397.0, -3856.0, -4286.0, -2740.0, -3660.0, -3355.0, -3261.0, -3887.0, -4805.0, -5319.0, -2885.0, -3182.0, -3944.0, -2453.0, -2111.0, -4341.0, -4797.0, -3512.0, -4118.0, -3867.0, -3771.0, -2385.0, -4522.0, -3627.0, -5440.0, -3311.0, -4782.0, -3481.0, -3850.0, -4019.0, -4524.0, -5461.0, -3025.0], 'policy_Agent2_reward': [-3876.0, -3622.0, -4420.0, -3081.0, -4840.0, -4115.0, -3275.0, -2852.0, -2989.0, -3813.0, -2657.0, -4295.0, -2110.0, -4382.0, -2654.0, -3448.0, -5272.0, -4371.0, -3578.0, -4143.0, -3306.0, -4316.0, -3954.0, -2882.0, -1400.0, -3730.0, -4892.0, -3879.0, -5348.0, -3768.0, -3596.0, -4757.0, -4488.0, -3780.0, -3570.0, -3475.0, -6745.0, -3316.0, -4059.0, -4999.0, -2721.0, -2721.0, -2704.0, -4319.0, -3830.0, -1945.0, -3638.0, -2928.0, -5639.0, -4237.0, -5330.0, -4283.0, -3656.0, -4139.0, -3461.0, -3610.0, -4178.0, -4391.0, -3334.0, -5193.0, -4126.0, -2697.0, -2740.0, -3154.0, -3733.0, -2907.0, -5113.0, -4397.0, -3856.0, -4286.0, -2740.0, -3660.0, -3355.0, -3261.0, -3887.0, -4805.0, -5319.0, -2885.0, -3182.0, -3944.0, -2453.0, -2111.0, -4341.0, -4797.0, -3512.0, -4118.0, -3867.0, -3771.0, -2385.0, -4522.0, -3627.0, -5440.0, -3311.0, -4782.0, -3481.0, -3850.0, -4019.0, -4524.0, -5461.0, -3025.0], 'policy_Agent4_reward': [-3876.0, -3622.0, -4420.0, -3081.0, -4840.0, -4115.0, -3275.0, -2852.0, -2989.0, -3813.0, -2657.0, -4295.0, -2110.0, -4382.0, -2654.0, -3448.0, -5272.0, -4371.0, -3578.0, -4143.0, -3306.0, -4316.0, -3954.0, -2882.0, -1400.0, -3730.0, -4892.0, -3879.0, -5348.0, -3768.0, -3596.0, -4757.0, -4488.0, -3780.0, -3570.0, -3475.0, -6745.0, -3316.0, -4059.0, -4999.0, -2721.0, -2721.0, -2704.0, -4319.0, -3830.0, -1945.0, -3638.0, -2928.0, -5639.0, -4237.0, -5330.0, -4283.0, -3656.0, -4139.0, -3461.0, -3610.0, -4178.0, -4391.0, -3334.0, -5193.0, -4126.0, -2697.0, -2740.0, -3154.0, -3733.0, -2907.0, -5113.0, -4397.0, -3856.0, -4286.0, -2740.0, -3660.0, -3355.0, -3261.0, -3887.0, -4805.0, -5319.0, -2885.0, -3182.0, -3944.0, -2453.0, -2111.0, -4341.0, -4797.0, -3512.0, -4118.0, -3867.0, -3771.0, -2385.0, -4522.0, -3627.0, -5440.0, -3311.0, -4782.0, -3481.0, -3850.0, -4019.0, -4524.0, -5461.0, -3025.0], 'policy_Agent1_reward': [-3876.0, -3622.0, -4420.0, -3081.0, -4840.0, -4115.0, -3275.0, -2852.0, -2989.0, -3813.0, -2657.0, -4295.0, -2110.0, -4382.0, -2654.0, -3448.0, -5272.0, -4371.0, -3578.0, -4143.0, -3306.0, -4316.0, -3954.0, -2882.0, -1400.0, -3730.0, -4892.0, -3879.0, -5348.0, -3768.0, -3596.0, -4757.0, -4488.0, -3780.0, -3570.0, -3475.0, -6745.0, -3316.0, -4059.0, -4999.0, -2721.0, -2721.0, -2704.0, -4319.0, -3830.0, -1945.0, -3638.0, -2928.0, -5639.0, -4237.0, -5330.0, -4283.0, -3656.0, -4139.0, -3461.0, -3610.0, -4178.0, -4391.0, -3334.0, -5193.0, -4126.0, -2697.0, -2740.0, -3154.0, -3733.0, -2907.0, -5113.0, -4397.0, -3856.0, -4286.0, -2740.0, -3660.0, -3355.0, -3261.0, -3887.0, -4805.0, -5319.0, -2885.0, -3182.0, -3944.0, -2453.0, -2111.0, -4341.0, -4797.0, -3512.0, -4118.0, -3867.0, -3771.0, -2385.0, -4522.0, -3627.0, -5440.0, -3311.0, -4782.0, -3481.0, -3850.0, -4019.0, -4524.0, -5461.0, -3025.0]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.766215894479135, 'mean_inference_ms': 1.9107422531764942, 'mean_action_processing_ms': 0.1736246614642831, 'mean_env_wait_ms': 19.216977280063816, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.01610221862792969, 'StateBufferConnector_ms': 0.0017731189727783203, 'ViewRequirementAgentConnector_ms': 0.03594498634338379}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1000000, 'num_agent_steps_trained': 1000000, 'num_env_steps_sampled': 200000, 'num_env_steps_trained': 200000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 61.40749630458054, 'num_env_steps_trained_throughput_per_sec': 61.40749630458054, 'timesteps_total': 200000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 1000000, 'timers': {'training_iteration_time_ms': 65862.717, 'sample_time_ms': 45939.883, 'learn_time_ms': 19913.709, 'learn_throughput': 200.867, 'synch_weights_time_ms': 3.461}, 'counters': {'num_env_steps_sampled': 200000, 'num_env_steps_trained': 200000, 'num_agent_steps_sampled': 1000000, 'num_agent_steps_trained': 1000000}, 'done': False, 'episodes_total': 400, 'training_iteration': 50, 'trial_id': 'default', 'date': '2024-04-17_02-24-21', 'timestamp': 1713335061, 'time_this_iter_s': 65.14513111114502, 'time_total_s': 3300.6404881477356, 'pid': 62022, 'hostname': 'COECIS-DM276492WC', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'env': 'CC4', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'validate_workers_after_construction': True, 'compress_observations': False, 'sampler_perf_stats_ema_coef': None, 'sample_async': -1, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'torch_action_mask_model', 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapper at 0x2b3118670>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': {'logdir': './logs/Masked_PPO_Example', 'type': 'ray.tune.logger.TBXLogger'}, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_enable_new_api_stack': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'Agent0': (None, Dict('action_mask': MultiBinary(82), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2])), Discrete(82), {'gamma': 0.85}), 'Agent1': (None, Dict('action_mask': MultiBinary(82), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2])), Discrete(82), {'gamma': 0.85}), 'Agent2': (None, Dict('action_mask': MultiBinary(82), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2])), Discrete(82), {'gamma': 0.85}), 'Agent3': (None, Dict('action_mask': MultiBinary(82), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2])), Discrete(82), {'gamma': 0.85}), 'Agent4': (None, Dict('action_mask': MultiBinary(242), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2])), Discrete(242), {'gamma': 0.85})}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 3300.6404881477356, 'iterations_since_restore': 50, 'perf': {'cpu_util_percent': 38.53478260869565, 'ram_util_percent': 83.22717391304347}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = algo_config.build()\n",
    "num_iters = 50\n",
    "for i in range(num_iters):\n",
    "    train_info=algo.train()\n",
    "\n",
    "algo.save(f'./Submissions/mask_results_{num_iters}_{time.strftime(\"%Y-%m-%d_%H:%M:%S\")}/staging/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': MultiBinary(82), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]))\n",
      "2\n",
      "Discrete(82)\n",
      "Box(-1.0, 1.0, (267,), float32)\n"
     ]
    }
   ],
   "source": [
    "policy = algo.get_policy(policy_id='Agent0')\n",
    "# policy.compute_actions()\n",
    "print(env.observation_space('blue_agent_0'))\n",
    "print(len(env.observation_space('blue_agent_0').sample()))\n",
    "print(env.action_space('blue_agent_0'))\n",
    "print(policy.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('action_mask', array([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "       0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "       0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0], dtype=int8)), ('observations', array([2, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
      "       0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "       0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
      "       1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0]))])\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space('blue_agent_0').sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': MultiBinary(242), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]))\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space('blue_agent_4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import PolicySpec, Policy\n",
    "my_restored_policy = Policy.from_checkpoint(\"./Submissions/mask_results_2024-04-16_02:01:29/staging/policies/Agent0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_env_creator_CC4(env_config: dict):\n",
    "    sg = EnterpriseScenarioGenerator(\n",
    "        blue_agent_class=SleepAgent,\n",
    "        green_agent_class=EnterpriseGreenAgent,\n",
    "        red_agent_class=FiniteStateRedAgent,\n",
    "        steps=500\n",
    "    )\n",
    "    cyborg = CybORG(scenario_generator=sg)\n",
    "    env = EnterpriseMAE(env=cyborg)\n",
    "    # env = EnterpriseMAEMaskWrapper(env=cyborg)\n",
    "    # env = MaskWrapper(env=cyborg)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env(name='CC4_PPO', env_creator=lambda config: ppo_env_creator_CC4(config))\n",
    "env = ppo_env_creator_CC4({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGENTS = 5\n",
    "POLICY_MAP = {f\"blue_agent_{i}\": f\"Agent{i}\" for i in range(NUM_AGENTS)}\n",
    "\n",
    "def policy_mapper(agent_id, episode, worker, **kwargs):\n",
    "# def policy_mapper(agent_id, episode, **kwargs):\n",
    "    return POLICY_MAP[agent_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "ppo_algo_config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"CC4_PPO\")\n",
    "    .framework(\"torch\")\n",
    "    .debugging(logger_config={'logdir':f'./tb_logs/PPO_Example_{timestamp}', 'type': 'ray.tune.logger.TBXLogger'})\n",
    "    .multi_agent(policies={\n",
    "        ray_agent: PolicySpec(\n",
    "            policy_class=None,\n",
    "            observation_space=env.observation_space(cyborg_agent),\n",
    "            action_space=env.action_space(cyborg_agent),\n",
    "            config={'gamma': 0.85}\n",
    "        ) for cyborg_agent, ray_agent in POLICY_MAP.items()\n",
    "    },\n",
    "    policy_mapping_fn=policy_mapper\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 09:40:57,934\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RomRoY6mjoKtQpkAy4Ghob9o3gUj+JRPIpH8WDAAN8+KDc=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RomRoY6mjoKtQpkAy4Ghgb7UTyKR/EoHsWjeDBgADBBVag=', 'shape': (421,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGhgY6mjoKtQpkAy5mBghgGqVH6VF6lB6lR+kBpAFVox0m', 'dtype': '<i8'}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGFqY6mjoKtQpkAy4Ghob9o3gUD2cMAKaKAhs=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGFqY6mjoKtQpkAy4Ghgb7UTyKhzMGAEo/pYw=', 'shape': (185,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGFqY6mjoKtQpkAy4Ghob9o3gUD2cMAKaKAhs=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGFqY6mjoKtQpkAy4Ghgb7UTyKhzMGAEo/pYw=', 'shape': (185,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGFqY6mjoKtQpkAy4Ghob9o3gUD2cMAKaKAhs=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGFqY6mjoKtQpkAy4Ghgb7UTyKhzMGAEo/pYw=', 'shape': (185,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGFqY6mjoKtQpkAy4Ghob9o3gUD2cMAKaKAhs=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqGFqY6mjoKtQpkAy4Ghgb7UTyKhzMGAEo/pYw=', 'shape': (185,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResult(checkpoint=Checkpoint(filesystem=local, path=./Submissions/results_2_2024-04-17_09:40:49/staging/), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'Agent4': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.9237068885316452, 'cur_kl_coeff': 0.29999999999999993, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.856803868214289, 'policy_loss': -0.0709566344313013, 'vf_loss': 9.921804336706797, 'vf_explained_var': -1.4175350467363993e-06, 'kl': 0.019853963697676478, 'entropy': 5.445857546230157, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 1440.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent1': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.9096082871779799, 'cur_kl_coeff': 0.29999999999999993, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.8861259440581, 'policy_loss': -0.040507729798264336, 'vf_loss': 9.921859556436539, 'vf_explained_var': 1.5029683709144592e-06, 'kl': 0.015913714107714208, 'entropy': 4.371582482258479, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 1440.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent2': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.8987444924190641, 'cur_kl_coeff': 0.29999999999999993, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.88125248750051, 'policy_loss': -0.04481030399668574, 'vf_loss': 9.921480691432953, 'vf_explained_var': -1.3831506172815959e-05, 'kl': 0.015273778183063181, 'entropy': 4.36989074498415, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 1440.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent3': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.9144589222967625, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.88580652376016, 'policy_loss': -0.04178557820850983, 'vf_loss': 9.924006508787473, 'vf_explained_var': -4.690885543823242e-06, 'kl': 0.017927989585162916, 'entropy': 4.367779207726319, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 1440.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent0': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.9427016641323765, 'cur_kl_coeff': 0.29999999999999993, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.88452570239703, 'policy_loss': -0.04314163512414477, 'vf_loss': 9.922476247946422, 'vf_explained_var': 4.2727837959925335e-06, 'kl': 0.017303604982442943, 'entropy': 4.363042557239533, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 1440.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 40000}, 'sampler_results': {'episode_reward_max': -18160.0, 'episode_reward_min': -36245.0, 'episode_reward_mean': -25698.4375, 'episode_len_mean': 499.0, 'episode_media': {}, 'episodes_this_iter': 8, 'policy_reward_min': {'Agent0': -7249.0, 'Agent1': -7249.0, 'Agent2': -7249.0, 'Agent3': -7249.0, 'Agent4': -7249.0}, 'policy_reward_max': {'Agent0': -3632.0, 'Agent1': -3632.0, 'Agent2': -3632.0, 'Agent3': -3632.0, 'Agent4': -3632.0}, 'policy_reward_mean': {'Agent0': -5139.6875, 'Agent1': -5139.6875, 'Agent2': -5139.6875, 'Agent3': -5139.6875, 'Agent4': -5139.6875}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-36245.0, -22695.0, -20080.0, -20060.0, -32470.0, -25380.0, -22845.0, -19015.0, -29000.0, -33160.0, -22515.0, -23865.0, -29620.0, -18160.0, -28290.0, -27775.0], 'episode_lengths': [499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499], 'policy_Agent0_reward': [-7249.0, -4539.0, -4016.0, -4012.0, -6494.0, -5076.0, -4569.0, -3803.0, -5800.0, -6632.0, -4503.0, -4773.0, -5924.0, -3632.0, -5658.0, -5555.0], 'policy_Agent1_reward': [-7249.0, -4539.0, -4016.0, -4012.0, -6494.0, -5076.0, -4569.0, -3803.0, -5800.0, -6632.0, -4503.0, -4773.0, -5924.0, -3632.0, -5658.0, -5555.0], 'policy_Agent2_reward': [-7249.0, -4539.0, -4016.0, -4012.0, -6494.0, -5076.0, -4569.0, -3803.0, -5800.0, -6632.0, -4503.0, -4773.0, -5924.0, -3632.0, -5658.0, -5555.0], 'policy_Agent3_reward': [-7249.0, -4539.0, -4016.0, -4012.0, -6494.0, -5076.0, -4569.0, -3803.0, -5800.0, -6632.0, -4503.0, -4773.0, -5924.0, -3632.0, -5658.0, -5555.0], 'policy_Agent4_reward': [-7249.0, -4539.0, -4016.0, -4012.0, -6494.0, -5076.0, -4569.0, -3803.0, -5800.0, -6632.0, -4503.0, -4773.0, -5924.0, -3632.0, -5658.0, -5555.0]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.701469591938889, 'mean_inference_ms': 1.7004299889520826, 'mean_action_processing_ms': 0.17137393425220382, 'mean_env_wait_ms': 17.378347223394925, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.01154780387878418, 'StateBufferConnector_ms': 0.0017219781875610352, 'ViewRequirementAgentConnector_ms': 0.03511369228363037}}, 'episode_reward_max': -18160.0, 'episode_reward_min': -36245.0, 'episode_reward_mean': -25698.4375, 'episode_len_mean': 499.0, 'episodes_this_iter': 8, 'policy_reward_min': {'Agent0': -7249.0, 'Agent1': -7249.0, 'Agent2': -7249.0, 'Agent3': -7249.0, 'Agent4': -7249.0}, 'policy_reward_max': {'Agent0': -3632.0, 'Agent1': -3632.0, 'Agent2': -3632.0, 'Agent3': -3632.0, 'Agent4': -3632.0}, 'policy_reward_mean': {'Agent0': -5139.6875, 'Agent1': -5139.6875, 'Agent2': -5139.6875, 'Agent3': -5139.6875, 'Agent4': -5139.6875}, 'hist_stats': {'episode_reward': [-36245.0, -22695.0, -20080.0, -20060.0, -32470.0, -25380.0, -22845.0, -19015.0, -29000.0, -33160.0, -22515.0, -23865.0, -29620.0, -18160.0, -28290.0, -27775.0], 'episode_lengths': [499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499], 'policy_Agent0_reward': [-7249.0, -4539.0, -4016.0, -4012.0, -6494.0, -5076.0, -4569.0, -3803.0, -5800.0, -6632.0, -4503.0, -4773.0, -5924.0, -3632.0, -5658.0, -5555.0], 'policy_Agent1_reward': [-7249.0, -4539.0, -4016.0, -4012.0, -6494.0, -5076.0, -4569.0, -3803.0, -5800.0, -6632.0, -4503.0, -4773.0, -5924.0, -3632.0, -5658.0, -5555.0], 'policy_Agent2_reward': [-7249.0, -4539.0, -4016.0, -4012.0, -6494.0, -5076.0, -4569.0, -3803.0, -5800.0, -6632.0, -4503.0, -4773.0, -5924.0, -3632.0, -5658.0, -5555.0], 'policy_Agent3_reward': [-7249.0, -4539.0, -4016.0, -4012.0, -6494.0, -5076.0, -4569.0, -3803.0, -5800.0, -6632.0, -4503.0, -4773.0, -5924.0, -3632.0, -5658.0, -5555.0], 'policy_Agent4_reward': [-7249.0, -4539.0, -4016.0, -4012.0, -6494.0, -5076.0, -4569.0, -3803.0, -5800.0, -6632.0, -4503.0, -4773.0, -5924.0, -3632.0, -5658.0, -5555.0]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.701469591938889, 'mean_inference_ms': 1.7004299889520826, 'mean_action_processing_ms': 0.17137393425220382, 'mean_env_wait_ms': 17.378347223394925, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.01154780387878418, 'StateBufferConnector_ms': 0.0017219781875610352, 'ViewRequirementAgentConnector_ms': 0.03511369228363037}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 40000, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 65.17552359887452, 'num_env_steps_trained_throughput_per_sec': 65.17552359887452, 'timesteps_total': 8000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 40000, 'timers': {'training_iteration_time_ms': 59667.937, 'sample_time_ms': 42455.787, 'learn_time_ms': 17198.199, 'learn_throughput': 232.582, 'synch_weights_time_ms': 5.067}, 'counters': {'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 40000}, 'done': False, 'episodes_total': 16, 'training_iteration': 2, 'trial_id': 'default', 'date': '2024-04-17_09-42-57', 'timestamp': 1713361377, 'time_this_iter_s': 61.38097286224365, 'time_total_s': 119.3563756942749, 'pid': 83232, 'hostname': 'COECIS-DM276492WC', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'env': 'CC4_PPO', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'validate_workers_after_construction': True, 'compress_observations': False, 'sampler_perf_stats_ema_coef': None, 'sample_async': -1, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapper at 0x2be71ca60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': {'logdir': './tb_logs/PPO_Example_2024-04-17_09:40:49', 'type': 'ray.tune.logger.TBXLogger'}, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_enable_new_api_stack': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'Agent0': (None, MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]), Discrete(82), {'gamma': 0.85}), 'Agent1': (None, MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]), Discrete(82), {'gamma': 0.85}), 'Agent2': (None, MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]), Discrete(82), {'gamma': 0.85}), 'Agent3': (None, MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]), Discrete(82), {'gamma': 0.85}), 'Agent4': (None, MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]), Discrete(242), {'gamma': 0.85})}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 119.3563756942749, 'iterations_since_restore': 2, 'perf': {'cpu_util_percent': 42.84137931034483, 'ram_util_percent': 82.73563218390804}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_algo = ppo_algo_config.build()\n",
    "num_iters = 2\n",
    "for i in range(num_iters):\n",
    "    train_info=ppo_algo.train()\n",
    "\n",
    "ppo_algo.save(f'./Submissions/results_{num_iters}_{timestamp}/staging/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cage-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CybORG import CybORG\n",
    "from CybORG.Simulator.Scenarios import EnterpriseScenarioGenerator\n",
    "from CybORG.Agents.Wrappers import EnterpriseMAE\n",
    "from CybORG.Agents import SleepAgent, EnterpriseGreenAgent, FiniteStateRedAgent\n",
    "from Wrappers import EnterpriseMAEMaskWrapper, MaskWrapper\n",
    "from CustomRLLib import TorchActionMaskModel, CustomModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import register_env\n",
    "from ray.rllib.algorithms.ppo import PPOConfig, PPO\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.models import ModelCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator_CC4(env_config: dict):\n",
    "    sg = EnterpriseScenarioGenerator(\n",
    "        blue_agent_class=SleepAgent,\n",
    "        green_agent_class=EnterpriseGreenAgent,\n",
    "        red_agent_class=FiniteStateRedAgent,\n",
    "        steps=500\n",
    "    )\n",
    "    cyborg = CybORG(scenario_generator=sg)\n",
    "    # env = EnterpriseMAE(env=cyborg)\n",
    "    # env = EnterpriseMAEMaskWrapper(env=cyborg)\n",
    "    env = MaskWrapper(env=cyborg)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env(name='CC4', env_creator=lambda config: env_creator_CC4(config))\n",
    "env = env_creator_CC4({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGENTS = 5\n",
    "POLICY_MAP = {f\"blue_agent_{i}\": f\"Agent{i}\" for i in range(NUM_AGENTS)}\n",
    "\n",
    "def policy_mapper(agent_id, episode, worker, **kwargs):\n",
    "# def policy_mapper(agent_id, episode, **kwargs):\n",
    "    return POLICY_MAP[agent_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"custom_pytorch_model\", CustomModel)\n",
    "ModelCatalog.register_custom_model(\"torch_action_mask_model\", TorchActionMaskModel)\n",
    "# ModelCatalog.register_custom_model(\"action_mask_model\", ActionMaskModel)\n",
    "\n",
    "\n",
    "# ray.init()\n",
    "# algo = ppo.PPO(env=\"CartPole-v1\", config={\n",
    "#     \"model\": {\n",
    "#         \"custom_model\": \"my_tf_model\",\n",
    "#         # Extra kwargs to be passed to your model's c'tor.\n",
    "#         \"custom_model_config\": {},\n",
    "#     },\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"CC4\")\n",
    "    .debugging(logger_config={'logdir':'./logs/PPO_Example', 'type': 'ray.tune.logger.TBXLogger'})\n",
    "    .multi_agent(policies={\n",
    "        ray_agent: PolicySpec(\n",
    "            policy_class=None,\n",
    "            observation_space=env.observation_space(cyborg_agent),\n",
    "            action_space=env.action_space(cyborg_agent),\n",
    "            config={'gamma': 0.85}\n",
    "        ) for cyborg_agent, ray_agent in POLICY_MAP.items()\n",
    "    },\n",
    "    policy_mapping_fn=policy_mapper\n",
    "    )\n",
    "    .training(\n",
    "        model={'custom_model': \"torch_action_mask_model\"}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(algo_config.policies['Agent0'].policy_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-16_17:16:53\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d_%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/cage-4/lib/python3.10/site-packages/ray/rllib/utils/from_config.py:197: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  object_ = constructor(*ctor_args, **ctor_kwargs)\n",
      "2024-04-16 17:16:56,971\tINFO worker.py:1724 -- Started a local Ray instance.\n",
      "\u001b[36m(RolloutWorker pid=46111)\u001b[0m 2024-04-16 17:16:59,685\tWARNING env.py:298 -- Your MultiAgentEnv <EnterpriseMAEMaskWrapper instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "2024-04-16 17:17:00,677\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n",
      "\u001b[36m(RolloutWorker pid=46111)\u001b[0m /Applications/anaconda3/envs/cage-4/lib/python3.10/site-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[36m(RolloutWorker pid=46111)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "2024-04-16 17:17:43,336\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "/Applications/anaconda3/envs/cage-4/lib/python3.10/site-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghob9o3gUj+JRTAgDAJnVaEo=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghgb7UTyKR/EoJoQBlgviuw==', 'shape': (267,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'dict', 'spaces': {'action_mask': {'space': 'multi-binary', 'n': 82}, 'observations': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpmZsY6mjoKtQpkAy4Ghob9o3gUj+JRPIpH8SgexaN4FFMDAwDaXlXc', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpmZsY6mjoKtQpkAy4Ghgb7UTyKR/EoHsWjeBSP4lE8iqmBAbBSCk0=', 'shape': (663,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'dict', 'spaces': {'action_mask': {'space': 'multi-binary', 'n': 242}, 'observations': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGhgY6mjoKtQpkAy5mBghgGqVH6VF6lB6lR+kBpAFVox0m', 'dtype': '<i8'}}}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghob9o3gUj+JRTAgDAJnVaEo=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghgb7UTyKR/EoJoQBlgviuw==', 'shape': (267,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'dict', 'spaces': {'action_mask': {'space': 'multi-binary', 'n': 82}, 'observations': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghob9o3gUj+JRTAgDAJnVaEo=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghgb7UTyKR/EoJoQBlgviuw==', 'shape': (267,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'dict', 'spaces': {'action_mask': {'space': 'multi-binary', 'n': 82}, 'observations': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}}}\n",
      "!!!! space_to_dict(self.observation_space):  {'space': {'space': 'box', 'low': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghob9o3gUj+JRTAgDAJnVaEo=', 'high': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2airqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RpGZuY6mjoKtQpkAy4Ghgb7UTyKR/EoJoQBlgviuw==', 'shape': (267,), 'dtype': '<f4'}, 'original_space': {'space': {'space': 'dict', 'spaces': {'action_mask': {'space': 'multi-binary', 'n': 82}, 'observations': {'space': 'multi-discrete', 'nvec': 'eJyb7BfqGxDJyFDGUK2eklqcXKRupaBuk2mhrqOgnpZfVFKUmBefX5SSChJ3S8wpTgWKF2ckFqQC+RqWRjqaOgq1CuQDLmYGCGAapUfpYUgDAGpCHDI=', 'dtype': '<i8'}}}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResult(checkpoint=Checkpoint(filesystem=local, path=./Submissions/results_2024-04-16_17:19:10/staging/), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'Agent3': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.8597488654466967, 'cur_kl_coeff': 0.29999999999999993, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.936818678180376, 'policy_loss': -0.02997081426656223, 'vf_loss': 9.962491466601689, 'vf_explained_var': -1.2999152143796285e-05, 'kl': 0.014326764248146735, 'entropy': 3.8729896634817123, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 1440.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent4': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.8690984778106212, 'cur_kl_coeff': 0.29999999999999993, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.911309420069058, 'policy_loss': -0.058836115999535345, 'vf_loss': 9.963830057779948, 'vf_explained_var': -7.4160595734914144e-06, 'kl': 0.02105150617917578, 'entropy': 4.9560042450825375, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 1440.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent2': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.8698606953956187, 'cur_kl_coeff': 0.29999999999999993, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.929852726062139, 'policy_loss': -0.038475171759879835, 'vf_loss': 9.962988250454266, 'vf_explained_var': -3.862896313269933e-05, 'kl': 0.017798849333689033, 'entropy': 3.9095797315239906, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 1440.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent0': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.7683125990753372, 'cur_kl_coeff': 0.29999999999999993, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.932359473903974, 'policy_loss': -0.03959303276399927, 'vf_loss': 9.96786462366581, 'vf_explained_var': -5.8931484818458554e-05, 'kl': 0.013626220086570264, 'entropy': 4.085401194294294, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 1440.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}, 'Agent1': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.8058473557233811, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 9.928738893071811, 'policy_loss': -0.04113806816797781, 'vf_loss': 9.965317205588024, 'vf_explained_var': -2.156862368186315e-05, 'kl': 0.022798823061467573, 'entropy': 3.9873248058060806, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 1440.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 40000}, 'sampler_results': {'episode_reward_max': -20570.0, 'episode_reward_min': -32775.0, 'episode_reward_mean': -25130.9375, 'episode_len_mean': 499.0, 'episode_media': {}, 'episodes_this_iter': 8, 'policy_reward_min': {'Agent4': -6555.0, 'Agent1': -6555.0, 'Agent0': -6555.0, 'Agent2': -6555.0, 'Agent3': -6555.0}, 'policy_reward_max': {'Agent4': -4114.0, 'Agent1': -4114.0, 'Agent0': -4114.0, 'Agent2': -4114.0, 'Agent3': -4114.0}, 'policy_reward_mean': {'Agent4': -5026.1875, 'Agent1': -5026.1875, 'Agent0': -5026.1875, 'Agent2': -5026.1875, 'Agent3': -5026.1875}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-29350.0, -21210.0, -27440.0, -22375.0, -22480.0, -28395.0, -21235.0, -26205.0, -22190.0, -20570.0, -32775.0, -24545.0, -25715.0, -26335.0, -30460.0, -20815.0], 'episode_lengths': [499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499], 'policy_Agent4_reward': [-5870.0, -4242.0, -5488.0, -4475.0, -4496.0, -5679.0, -4247.0, -5241.0, -4438.0, -4114.0, -6555.0, -4909.0, -5143.0, -5267.0, -6092.0, -4163.0], 'policy_Agent1_reward': [-5870.0, -4242.0, -5488.0, -4475.0, -4496.0, -5679.0, -4247.0, -5241.0, -4438.0, -4114.0, -6555.0, -4909.0, -5143.0, -5267.0, -6092.0, -4163.0], 'policy_Agent0_reward': [-5870.0, -4242.0, -5488.0, -4475.0, -4496.0, -5679.0, -4247.0, -5241.0, -4438.0, -4114.0, -6555.0, -4909.0, -5143.0, -5267.0, -6092.0, -4163.0], 'policy_Agent2_reward': [-5870.0, -4242.0, -5488.0, -4475.0, -4496.0, -5679.0, -4247.0, -5241.0, -4438.0, -4114.0, -6555.0, -4909.0, -5143.0, -5267.0, -6092.0, -4163.0], 'policy_Agent3_reward': [-5870.0, -4242.0, -5488.0, -4475.0, -4496.0, -5679.0, -4247.0, -5241.0, -4438.0, -4114.0, -6555.0, -4909.0, -5143.0, -5267.0, -6092.0, -4163.0]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7467631000460115, 'mean_inference_ms': 1.9158918925299209, 'mean_action_processing_ms': 0.1717359323483508, 'mean_env_wait_ms': 17.55044842767215, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.01642972230911255, 'StateBufferConnector_ms': 0.0018319487571716309, 'ViewRequirementAgentConnector_ms': 0.03622174263000488}}, 'episode_reward_max': -20570.0, 'episode_reward_min': -32775.0, 'episode_reward_mean': -25130.9375, 'episode_len_mean': 499.0, 'episodes_this_iter': 8, 'policy_reward_min': {'Agent4': -6555.0, 'Agent1': -6555.0, 'Agent0': -6555.0, 'Agent2': -6555.0, 'Agent3': -6555.0}, 'policy_reward_max': {'Agent4': -4114.0, 'Agent1': -4114.0, 'Agent0': -4114.0, 'Agent2': -4114.0, 'Agent3': -4114.0}, 'policy_reward_mean': {'Agent4': -5026.1875, 'Agent1': -5026.1875, 'Agent0': -5026.1875, 'Agent2': -5026.1875, 'Agent3': -5026.1875}, 'hist_stats': {'episode_reward': [-29350.0, -21210.0, -27440.0, -22375.0, -22480.0, -28395.0, -21235.0, -26205.0, -22190.0, -20570.0, -32775.0, -24545.0, -25715.0, -26335.0, -30460.0, -20815.0], 'episode_lengths': [499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499], 'policy_Agent4_reward': [-5870.0, -4242.0, -5488.0, -4475.0, -4496.0, -5679.0, -4247.0, -5241.0, -4438.0, -4114.0, -6555.0, -4909.0, -5143.0, -5267.0, -6092.0, -4163.0], 'policy_Agent1_reward': [-5870.0, -4242.0, -5488.0, -4475.0, -4496.0, -5679.0, -4247.0, -5241.0, -4438.0, -4114.0, -6555.0, -4909.0, -5143.0, -5267.0, -6092.0, -4163.0], 'policy_Agent0_reward': [-5870.0, -4242.0, -5488.0, -4475.0, -4496.0, -5679.0, -4247.0, -5241.0, -4438.0, -4114.0, -6555.0, -4909.0, -5143.0, -5267.0, -6092.0, -4163.0], 'policy_Agent2_reward': [-5870.0, -4242.0, -5488.0, -4475.0, -4496.0, -5679.0, -4247.0, -5241.0, -4438.0, -4114.0, -6555.0, -4909.0, -5143.0, -5267.0, -6092.0, -4163.0], 'policy_Agent3_reward': [-5870.0, -4242.0, -5488.0, -4475.0, -4496.0, -5679.0, -4247.0, -5241.0, -4438.0, -4114.0, -6555.0, -4909.0, -5143.0, -5267.0, -6092.0, -4163.0]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7467631000460115, 'mean_inference_ms': 1.9158918925299209, 'mean_action_processing_ms': 0.1717359323483508, 'mean_env_wait_ms': 17.55044842767215, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.01642972230911255, 'StateBufferConnector_ms': 0.0018319487571716309, 'ViewRequirementAgentConnector_ms': 0.03622174263000488}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 40000, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 64.12094320773362, 'num_env_steps_trained_throughput_per_sec': 64.12094320773362, 'timesteps_total': 8000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 40000, 'timers': {'training_iteration_time_ms': 64852.934, 'sample_time_ms': 41231.251, 'learn_time_ms': 23603.876, 'learn_throughput': 169.464, 'synch_weights_time_ms': 6.319}, 'counters': {'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 40000}, 'done': False, 'episodes_total': 16, 'training_iteration': 2, 'trial_id': 'default', 'date': '2024-04-16_17-19-10', 'timestamp': 1713302350, 'time_this_iter_s': 62.396406173706055, 'time_total_s': 129.7307779788971, 'pid': 46073, 'hostname': 'COECIS-DM276492WC', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'env': 'CC4', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'validate_workers_after_construction': True, 'compress_observations': False, 'sampler_perf_stats_ema_coef': None, 'sample_async': -1, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'torch_action_mask_model', 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapper at 0x2b7a1c8b0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': {'logdir': './logs/PPO_Example', 'type': 'ray.tune.logger.TBXLogger'}, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_enable_new_api_stack': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'Agent0': (None, Dict('action_mask': MultiBinary(82), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2])), Discrete(82), {'gamma': 0.85}), 'Agent1': (None, Dict('action_mask': MultiBinary(82), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2])), Discrete(82), {'gamma': 0.85}), 'Agent2': (None, Dict('action_mask': MultiBinary(82), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2])), Discrete(82), {'gamma': 0.85}), 'Agent3': (None, Dict('action_mask': MultiBinary(82), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2])), Discrete(82), {'gamma': 0.85}), 'Agent4': (None, Dict('action_mask': MultiBinary(242), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
       " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2])), Discrete(242), {'gamma': 0.85})}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 129.7307779788971, 'iterations_since_restore': 2, 'perf': {'cpu_util_percent': 49.740449438202255, 'ram_util_percent': 81.90224719101123}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = algo_config.build()\n",
    "\n",
    "for i in range(2):\n",
    "    train_info=algo.train()\n",
    "\n",
    "algo.save(f'./Submissions/results_{time.strftime(\"%Y-%m-%d_%H:%M:%S\")}/staging/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': MultiBinary(82), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]))\n",
      "2\n",
      "Discrete(82)\n",
      "Box(-1.0, 1.0, (267,), float32)\n"
     ]
    }
   ],
   "source": [
    "policy = algo.get_policy(policy_id='Agent0')\n",
    "# policy.compute_actions()\n",
    "print(env.observation_space('blue_agent_0'))\n",
    "print(len(env.observation_space('blue_agent_0').sample()))\n",
    "print(env.action_space('blue_agent_0'))\n",
    "print(policy.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('action_mask', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0], dtype=int8)), ('observations', array([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "       1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "       1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "       0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0]))])\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space('blue_agent_0').sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': MultiBinary(242), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]))\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space('blue_agent_4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import PolicySpec, Policy\n",
    "my_restored_policy = Policy.from_checkpoint(\"./Submissions/mask_results_2024-04-16_02:01:29/staging/policies/Agent0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cage-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
